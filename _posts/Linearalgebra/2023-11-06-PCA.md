---
layout: single
title: "PCA"
categories: linearalgebra
tags: [linear algebra, PCA]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true


---

*Principal Component Analysis*





- 개요 [https://www.youtube.com/watch?v=FhQm2Tc8Kic](https://www.youtube.com/watch?v=FhQm2Tc8Kic)

- 자세한 수식 유도 [https://www.youtube.com/watch?v=C21GoH0Y9AE&t=91](https://www.youtube.com/watch?v=C21GoH0Y9AE&t=91)



# 종류

- Supervised feature selection: information gain, stepwise regression, LASSO, Genetic algorithm, many more...
- Supervised feature extraction: Partial least squares (PLS)
- Unsupervised feature selection: PCA loading
- Unsupervised feature extraction: Principal component analysis (PCA), Wavelets transforms, Autoencoder



# 개요

- 고차원 데이터를 효과적으로 분석하기 위한 대표적 분석 기법
- 차원축소, 시각화, 군집화, 압축

- PCA는 n개의 관측치와 p개의 변수로 구성된 데이터를 상관관계가 없는 k개의 변수로 구성된 데이터 (n개의 관측치)로 요약하는 방식으로, 이 때 요약된 변수는 기존 변수의 '선형 조합'으로 생성됨

- 원래 데이터의 분산을 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 사영 Projection 시키는 기법
- 주요 목적
  - 데이터 차원 축소 (n by p -> n by k, where k << p)
  - 데이터 시각화 및 해석
- 일반적으로 PCA는 전체 분석 과정 중 초기에 사용



z1 = a1X = a11X1 + a12X2 + ... + a1pXp

z2 =  a2X = a21X1 + a22X2 + ... + a2pXp

 zp = apX = ap1X1 + ap2X2 + ... + appXp



X1, X2, Xp : 원래 변수

ai i 번째 기저 basis 또는 계수 loading

Z1, Z2, ... , Zp 각 기저로 사영된 변환 후 변수. 주성분, score



분산을 최대화할 수 있는 사영 축을 찾는다





시간 16:33~ 본론

다변량 데이터에 대해서,

평균, Covariance(공분산, 분산), Correlation(스케일링한 covariance라고 보면 됨) 행렬을 구할 수 있다

분산의 곱을해서 제곱근으로 나누면 correlation을 알 수 있다
