---
layout: single
title: "AI Deep Dive, Chapter 3. 왜 우리는 인공신경망을 공부해야 하는가? 06. Mini-batch SGD"
categories: machinelearning
tags: [ML, Machine Learning, AI, AI Deep Dive]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true


---

*AI Deep Dive Note*



# Chapter 3 - 06. Mini-batch SGD



- SGD: 하나만 보는 건 너무 성급한 거 아니냐? -> mini batch! epoch별로 2개씩 보자

![ch0306_1]({{site.url}}/images/$(filename)/ch0306_1.png)



- GPU는 병렬연산이 가능하여 여러 데이터에 대해서도 빠르다

![ch0306_2]({{site.url}}/images/$(filename)/ch0306_2.png)

- 미니 배치 사이즈를 키울 수록 좋을까? 8k까지만!

![ch0306_3]({{site.url}}/images/$(filename)/ch0306_3.png)

[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://aiichironakano.github.io/cs653/Goyal-LargeMinibatchSGD-arXiv17.pdf)

- 제안 1. batch size를 두 배 키웠으면, learning rate도 두 배 키워라

- 제안 2. warm-up



# 강의 내용 중, 용어 설명

- Epoch
- Batch size

![ch0306_4]({{site.url}}/images/$(filename)/ch0306_4.png)





# 논문요약

### 요약:

#### 목표:

- 이 논문은 딥러닝에서 더 큰 네트워크와 데이터셋과 관련된 긴 훈련 시간을 해결하려고 합니다.
- 분산 동기 SGD와 큰 미니배치 크기를 사용하여 모델 정확도를 유지하고 훈련 시간을 줄이는 방법을 탐구합니다.

#### 도전 과제:

- 더 큰 네트워크와 데이터셋은 더 긴 훈련 시간을 초래합니다.
- ImageNet 데이터셋에서 큰 미니배치를 사용하는 것은 최적화에 어려움을 초래합니다.
- 큰 단위의 작업량과 정확도를 유지하면서 효율성을 달성하는 것이 어렵습니다.

#### 해결책 및 기술:

- **선형 스케일링 규칙:** 학습률은 미니배치 크기와 동일한 요인으로 곱해집니다. 이 규칙은 다양한 미니배치 크기에 대해 효과적이며 훈련과 일반화 정확도를 유지하는 데 도움이 됩니다.

- 웜업 전략:

   훈련 초기에 낮은 학습률을 사용하여 초기 최적화 어려움을 극복하는 전략입니다. 두 가지 웜업이 논의되었습니다:

  - **상수 웜업:** 훈련의 처음 몇 에포크 동안 낮은 상수 학습률을 사용합니다.
  - **점진적 웜업:** 학습률을 점진적으로 작은 값에서 큰 값으로 높여, 훈련 시작 시 건강한 수렴을 허용합니다.

- **큰 미니배치와 배치 정규화 (BN):** 각 샘플의 손실의 독립성을 유지하고 미니배치 크기를 변경할 때 BN의 영향을 논의합니다.

#### 결과:

- 논문은 최대 8192 이미지의 큰 미니배치 크기로 훈련할 때 정확도의 손실이 없음을 보여줍니다.
- 이 기술을 사용하여 256개의 GPU에서 미니배치 크기가 8192인 ResNet-50을 한 시간 안에 훈련할 수 있었으며, 작은 미니배치 정확도와 일치했습니다.
- 구현은 8개에서 256개의 GPU로 이동할 때 약 90%의 스케일링 효율성을 달성합니다.

#### 실용적인 함의:

- 이 기술은 인터넷 규모의 데이터로 시각 모델을 훈련하는 데 유용하며, 하이퍼파라미터 검색 없이 알고리즘을 단일 GPU에서 다중 GPU 구현으로 마이그레이션하는 것을 단순화합니다.





## 논문 내 주요 기술용어

1. **Large Minibatch SGD (큰 미니배치 확률적 경사 하강법)**

   - 큰 미니배치를 사용하여 모델을 훈련하는 확률적 경사 하강법입니다. 이 방법은 훈련 시간을 줄이기 위해 사용됩니다.

2. **Linear Scaling Rule (선형 스케일링 규칙)**

   - 미니배치 크기와 동일한 요인으로 학습률을 곱하는 규칙입니다. 이 규칙은 다양한 미니배치 크기에 대해 효과적입니다.

   - 선형 스케일링 규칙은 미니배치 크기가 커짐에 따라 학습률을 증가시켜, 각 에폭에서 더 적은 업데이트가 이루어지더라도 전체 학습 과정에서 충분한 학습이 이루어질 수 있도록 합니다.

     이 규칙을 통해 미니배치 크기를 크게 하면서도 모델의 수렴을 유지하고, 학습 시간을 단축할 수 있습니다.

3. **Warmup Strategy (웜업 전략)**

   - 훈련 초기에 낮은 학습률을 사용하여 초기 최적화 어려움을 극복하는 전략입니다.

4. **Constant Warmup (상수 웜업)**

   - 훈련의 처음 몇 에포크 동안 낮은 상수 학습률을 사용하는 웜업 방법입니다.

5. **Gradual Warmup (점진적 웜업)**

   - 학습률을 점진적으로 작은 값에서 큰 값으로 높여, 훈련 시작 시 건강한 수렴을 허용하는 웜업 방법입니다.

6. **Batch Normalization (배치 정규화)**

   - 각 미니배치에서 입력을 정규화하여 학습을 안정화하고 가속화하는 기술입니다.
   - 논문 [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

7. **Distributed Synchronous SGD (분산 동기 확률적 경사 하강법)**

   - 여러 기기에서 동시에 모델을 훈련하는 확률적 경사 하강법입니다. 이 방법은 훈련 시간을 줄이기 위해 사용됩니다.
   - 논문 [Revisiting Distributed Synchronous SGD](https://arxiv.org/pdf/1604.00981.pdf)
     - 동기 확률적 경사 하강법을 재검토
     - 동기 확률 최적화에서 지체를 완화하는 방법 제안
     - 여러 백업 기계를 사용하여 동기 병렬 SGD를 제안
     - 분산 동기 SGD는 여러 서버에서 모델의 매개변수를 분산시킬 수 있으며, 이는 TensorFlow 시스템을 사용하여 실험됨

8. **Scaling Efficiency (스케일링 효율성)**

   - 여러 기기를 사용할 때 훈련 효율성이 얼마나 잘 유지되는지를 나타내는 지표입니다.

9. **ResNet-50 (레즈넷-50)**

   - 50개의 층을 가진 Residual Network의 모델입니다. 이 모델은 이미지 분류 작업에 널리 사용됩니다.

10. **ImageNet (이미지넷)**

    - 대규모 이미지 데이터셋으로, 다양한 객체와 개념을 포함하고 있어 컴퓨터 비전 연구에 널리 사용됩니다.





## 무조건 8k를 쓰는 게 맞을까?

그냥 쓰기보다는 모델의 성능과 정확도를 주기적으로 확인하며 적절한 minibatch 크기와 학습률을 결정해야 한다

- 모델의 구조, 데이터셋의 특성에 따라 적절한 크기가 달라질 수
- 정확도와 수렴 속도 - 큰 크기는 훈련시간을 줄일 수 있지만 수렴 속도와 최종 정확도에 영향
- 컴퓨팅 리소스 (GPU 메모리)
- 하이퍼 파라미터 튜닝 (학습률, 가중치 초기화, 옵티마이저 등)
- 과적합 문제 - 미니 배치 사이즈가 너무 작으면 과적합될 수. 너무 크면 수렴 문제를 일으킬 수.









# 미니배치 설명

미니배치 크기(Mini-batch Size)는 딥 러닝에서 학습 데이터를 나눠서 처리하는 방법 중 하나로, 전체 데이터셋을 한 번에 처리하는 것이 아니라 작은 미니배치 단위로 데이터를 나누어 학습합니다. 미니배치 크기는 학습 알고리즘의 하이퍼파라미터 중 하나로 설정되며, 이 크기를 조절하여 학습 과정의 속도와 안정성을 조절할 수 있습니다.

미니배치 크기의 장점은 다음과 같습니다:

1. 학습 속도 향상: 전체 데이터셋을 한 번에 처리하는 것보다 미니배치 단위로 나눠서 처리하면 학습 과정이 빨라집니다. 특히 대규모 데이터셋에서 이점을 얻을 수 있습니다.
2. 메모리 효율성: 전체 데이터를 메모리에 한 번에 로드할 필요가 없으므로 메모리 효율적으로 학습할 수 있습니다.
3. 더 나은 일반화: 미니배치 학습은 데이터를 무작위로 섞는 효과를 가지므로 모델이 전체 데이터에 과적합되는 것을 방지하고 일반화 성능을 향상시킬 수 있습니다.

GPU 병렬 연산은 그래픽 처리 장치(GPU)를 사용하여 딥 러닝 모델의 학습 및 추론 과정을 가속화하는 기술입니다. GPU는 병렬 처리 능력이 뛰어나기 때문에 많은 수치 연산을 동시에 처리할 수 있어 딥 러닝 모델의 학습 속도를 향상시킵니다.

미니배치 크기와 GPU 병렬 연산은 학습 속도와 메모리 사용량을 관리하고 모델 성능을 향상시키는 데 중요한 역할을 합니다. 대규모 데이터셋과 복잡한 모델의 경우 적절한 미니배치 크기와 GPU 가용성을 고려하여 학습을 조정하면 학습 과정이 효율적으로 진행됩니다.