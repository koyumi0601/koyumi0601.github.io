---
layout: single
title: "ResNet - loss landscape"
categories: machinelearning
tags: [ML, Machine Learning, AI, Legend13]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
---

*Legend 13 (Image)  Voice Transciption*



# Paper 

- Visualizing the Loss Landscape of Neural Nets [https://arxiv.org/pdf/1712.09913.pdf](https://arxiv.org/pdf/1712.09913.pdf)

- Paper summary

1. **필터별 정규화(Filter-Wise Normalization)**: 이 논문은 필터별 정규화라는 시각화 방법을 소개합니다. 이 방법은 무작위 가우시안 방향 벡터를 사용하여 손실 함수를 플로팅하며, 이 벡터들은 신경망 매개변수의 스케일에 맞게 정규화됩니다. 이 접근법은 손실 함수의 곡률을 시각화하는 데 도움이 되며, 다른 손실 함수 간에 의미 있는 비교를 가능하게 합니다.
2. **1차원 선형 보간(1-Dimensional Linear Interpolation)**: 이 방법은 두 세트의 매개변수를 선으로 연결하고 이 선을 따라 손실 값을 플로팅하는 간단한 방법입니다. 이 방법은 다양한 최소값의 날카로움과 평탄함을 연구하는 데 사용되었습니다.
3. **등고선 플롯 & 무작위 방향(Contour Plots & Random Directions)**: 이 방법은 중심점과 두 방향 벡터를 선택하여 1차원 또는 2차원에서 손실 함수를 플로팅합니다. 이 접근법은 다양한 최소화 방법의 궤적을 탐색하고 다른 최적화기가 다른 국소 최소값을 찾는다는 것을 보여주는 데 사용되었습니다.
4. **날카로움 대 평탄함(Sharp vs Flat Dilemma)**: 논문은 날카로운 최소화기가 평탄한 것보다 일반화가 더 잘되는지에 대한 논쟁을 다룹니다. 필터 정규화로 시각화할 때 최소화기의 날카로움이 일반화 오류와 잘 상관한다는 것을 제안합니다.
5. **스케일 불변성(Scale Invariance)**: 논문은 또한 ReLU 비선형성과 배치 정규화를 사용할 때 신경망의 스케일 불변성 문제를 다룹니다. 적절한 정규화 없이는 손실 함수 플롯 간의 비교가 오해의 소지가 있다고 강조합니다.
6. **일반화와 네트워크 구조(Generalization and Network Architecture)**: 논문은 스킵 연결과 같은 네트워크 구조가 손실 풍경과 일반화에 어떤 영향을 미치는지 탐구합니다.

문서는 손실 풍경의 시각화가 신경망의 행동을 이해하고 다양한 구조와 학습 매개변수가 모델의 학습 가능성과 일반화에 미치는 영향을 이해하는 데 강력한 도구라고 강조합니다.






# Transcription
- 클로바노트 https://clovanote.naver.com/

##### 받아쓰기


오케이

그러면 재민 님은요 오케이 내려보겠습니다.

우리 두 분과 함께해 보도록 하겠습니다.

일단 레즈넷을 본격적으로 하기 전에 레즈넷이 어떤 걸 해결했나 그것부터 그 문제가 있거든요.
유명한 문제 그 문제가 뭔지 그것부터 한번 출발해 볼까 합니다.
로스 앤드 스케이라는 건데 로스 앤드 스케이이라는 논문이 나왔었어요.
그래서 이 논문에서 하고 싶은 말은 뭐냐면 이런 겁니다.
일단 로스의 모양을 보려고 하는 게 로스 랜드스케이이고 그야말로 이런 문제가 있어도 돼요.
렐로랑 베치농 같이 쓰면 그라디언트가 충분히 커집니다.
베니싱 그라디언트가 해결이 된 셈이죠. 그런데도 불구하고 너무 기쁘면 학습이 안 되더라라는 거예요.
언더 피팅이 일어나는데 이런 식으로 애매하게 언더패팅이 일어난다.
그러니까 베니싱 그라디언트가 일어나잖아요. 그러면 뭐 이렇게 뜨문뜨문하다가 떨어지지도 않아요.
떨어지지도 않아요. 그냥 맨 위에서 그냥 놀아요.
베니싱 그래터가 일어나면 아예 학습 자체가 안 돼요.
근데 희한하게 한번 그래프 봅시다. 트레이닝 에러 테스트 에러 둘 다 누가 더 못합니까? 학습을 둘 다

깊은 증입니다.

깊은 놈 56층을 가진 녀석이 테스트뿐만 아니고 트레이닝 도 못하더라.
만약에 트레이닝은 잘하는데 테스트 때 못하면 그거는 무슨 문제라고 얘기를 해요 우리가

그렇죠 그거는 오버 피팅인 건데 여기서 나온 문제는 우버 피팅 말고 다른 문제다라는 겁니다.

원인은 못 밝혀냈어요. 저 왼쪽에서 비겨 1 타이밍 성함 이렇게 써 있죠 저 분이 바로 레즈넷 만든 분이고 레즈넷 페이퍼의 피겨 원이에요.
저게 딥 레즈듀얼 러닝 이렇게 써 있잖아요. 저게 레즈넷 논문입니다.
저런 문제를 발견했는데 왜 저런 일이 일어났는지까지는 그 논문에서 밝혀내지는 못했습니다.
그렇지만 나중에 밝혀지기를 이렇게 모양이 좀 꾸불꾸불해지더라라는 거예요.
저기 스키 커넥션 이렇게 써 있잖아요. 레즈넷의 스키 커넥션이 이 꼬불꼬불해지는 문제를 해결해 준 녀석인 건데 오른쪽 아래 논문은 카이번의 논문 아닙니다.
2018년도의 논문이죠. 그러니까 2016년도에 레즈넷이 나왔어요.
그래서 깊은데 이상하게 언더패팅이 일어나는 그 문제를 해결했습니다.
근데 뭐가 문제인지는 못 밝혀냈고 2018년에 돼서야 아마 로스 랜드스케입이 꼬불거리기 때문에 문제가 된 것 같다.
이렇게 한 거예요. 그러니까 현상을 밝힌 거죠. 레이어를 깊게 깊게 쌓였더니 56층쯤 되니까 꼬불꼬불하더라라는 겁니다.
아무래도 꼬불꼬불하면 그렇죠 학습이 어렵겠죠 좀 다른 데 들어갈 수도 있는 거고 이해돼요 무슨 말인지 루스의 모양이 이런 식으로 생겼으면 다른 데 어디 기툰 이런 데가 있을 수도 있는데 너무 꼬불꼬불거리고 있으니까 여기서 출발해서 아담을 한다고 하더라도 이런 데서 빠진다는 거예요.
여기가 로컬 미니멈인 거예요. 여긴 더 좋은 더 좋은 로컬 매니 글로벌 글로벌이 여기 있다 쳐요.
여기를 가고 싶었을 텐데 못 가고 이런 데 빠져서 못 나오더라라는 거.

그래서 이렇게 로스의 모양을 봄으로써 어떤 모델 모델의 학습이 잘 될까 안 될까를 가늠해 보는 것이 가능해진 거예요.
이 논문 덕분에 로스 랜드스케 노무 덕분에 근데 이 레즈넷에서 제안한 스키 커넥션 을 적용했더니 적용하고 로스의 모양을 봤더니 평평해요.
그죠? 여기에서 출발하더라도 이렇게 쭉 들어와서 일로 잘 빠질 수가 있겠다라는 거.

스키 커미션이 뭔지를 우리가 아는 게 레즈넷을 이해하는 것이고 이 로스 엔디스케이에서는 봐라 스키 커넥션이라는 것을 적용했더니 이렇게 모양이 예뻐지지 않느냐라는 당연히 얘보다 얘가 성능이 좋겠죠 딱 봐도 그거는 그렇습니다.
이 모양을 어떻게 그려냈을까 그걸 제안한 게 바로 이 논문인 거예요.
이렇게 그림을 어떻게 그리는지 로스를 우리가 그림을 그릴 수가 있을까요? 3차원에다가 그냥 웨이트의 함수로 그것부터 이은 목까? 그냥은 못 그리죠.
왜죠? 차원이 달라서요. 차원이 수천만 수 100만 차원에 놓이는 거잖아요.
로스라는 게. 이해돼요 무슨 말인지 웨이트가 수백만 개인데 축이 하나면 이렇게 2d로 그림이 그려지고 축이 두 개면 이렇게 3d로 그려질 텐데 축이 몇백만 개인 거예요.
어떻게 그런 그림을 그리냐 바로 얘가 아주 기가 막힌 아이디어를 냈는데 일단 있었던 아이디어가 있어요.
원래 있었던 아이디어는 랜덤 백터 2개를 뽑아라.
웨이트 벡터랑 똑같은 사이즈를 가진 랜덤 벡터 2개를 뽑아라.
그래가지고 알파랑 베타를 마이너스 1에서 1까지 바꿔가면서 그림을 그려라.
이렇게 제안된 바가 있습니다. 근데 그렇게 그림을 그렸더니 좀 이상해져가지고 뭔가 짱구를 굴려가지고 뭔가 처리를 특별한 처리를 한 랜덤 벡터를 가지고 이렇게 스케닝을 하자라는 거예요.
그러니까 뭔 말이냐 세타얘는 로컬 미니멈에 해당되는 웨이트인 거예요.
그 웨이트가 100만 개 있다 쳐요. 그러면 얘의 길이는 어떻게 됩니까?

웨이트 1 웨이트 e 몇 개가 되는 거예요? 웨이트가 100만 개면

10만 개

100만 개 근데 이 웨이트가 cnn의 웨이트라고 합시다.
그러면 여기가 첫 번째 레이어에 첫 번째 필터 그리고 여기가 첫 번째 레이어에 두 번째 필터

저기

뭐 이상한 점 있으세요? 질문 질문 있었어요 아닌가 이렇게 쭉 되겠죠 이해돼요? 무슨 말인지 웨이트가 첫 번째 레이어의 첫 번째 필터 첫 번째 레이어에 두 번째 필터 그리고 마지막 레이어에 마지막 필터 이렇게 웨이트 값이 쭈르르르륵 있을 겁니다.
맞습니까? 이해됐어요? 입자 사이트 한번 들어와서 눌러주시고 노트 협 오케이.
저도 계속 보고 있어요. 오케이 좋습니다. 그런데 짱구를 굴렸다 그랬어요.
어떻게 굴렸냐 이거를 이거를 각각의 축으로 해가지고 그림을 그릴 수가 있다 없다? 없기 때문에 그냥 랜덤한 벡터를 만들어라.
다만 그 길이가 100만 개인 100만에 해당되는 랜덤한 웨이트 벡터를 만들어라라는 거예요.
이렇게 이거 랜덤 값이에요. 그냥 랜덤 값. 이게 100만 개 이게 델타 델타 에타 같은 방식으로 만드는 겁니다.
델타만 생각하시면 돼요. 델타 에타 똑같은 방식으로 만들 겁니다.
그랬을 때 델타는 랜덤한 값으로 하나 벡터를 만든다.
100만 개짜리. 그러고 나서 이 델타랑 에타를 만들고 이 벡터를 축으로 스케일을 시키는 거죠.
여기다가 알파 여기다가 베타 하면은 그러면 말하자면 어떤 로스가 있을 때 100만 차원 위에 놓인 롤스가 있겠죠 이게 100만 차원인 건데 여기 여기가 100만 차원 위에다 이제 그림이 그려지겠죠 거기에 델타 에타가 있는 거고 근데 델타 에타를 바꿔가면서 그림을 그리면 그 바꿔가는 정도를 알파랑 베타로 놓고 3차원 위에 그림을 그리자라는 거예요.
알파랑 베타를 바꿔가면서 l 값을 보자. 대신 그 축 자체는 데이터랑 에타 축인 거예요.
데이터랑 에타 축으로 쭉쭉 해가지고 로스의 모양을 그리자.
알파만큼 가고 델타만큼 가서 로스가 그리고 그럼 알파베타가 0이면 그때는 어떻게 되는 거예요? 로컬 미니멈 값에 해당되는 로스를 보는 거죠.
이해돼요? 무슨 말인지 알파랑 메타가 0이면 됐어요 알파랑 메타가 0이면 그냥 로컬 미니멈 값을 보게 될 것이다.
여기서 관건입니다.

이렇게 옛날에 했었는데 스케일링 문제가 좀 있었대요.
그래가지고 얘네가 어떻게 생각했냐면 여기에 필터 있죠 사이즈가 있을 거잖아요.
첫 번째 필터 사이즈가 거기에 해당되는 똑같은 사이즈를 가진 얘네들을 얘네 크기랑 얘네 크기랑 똑같이 맞춘 거예요.
여기의 크기랑 여기의 크기랑 이해돼요? 무슨 말인지 요 벡터의 크기랑 이 랜덤 벡터의 크기랑 일치하게끔 맞춰준 겁니다.
어떻게 맞춰주냐 여기를 델타아이마 델타 1 1이라고 합시다.
1 1 1 콤마 1 첫 번째 레이아웃 첫 번째 설정이 있다.
이게 벡터예요. 그러면 이 벡터를 일단 노말라이즈를 하는 거예요.
이렇게 투노머로. 그러면 얘는 크기가 몇이에요?

예

1이죠. 여기다가 이게 웨이트 1 콤마 1이라고 하면 1 콤마 1 벡터 그러면 여기다가 이만큼을 곱해주죠.
델타 세타 별 일 콤마일이죠 델타 별 일 콤마일

그러면 얘는 크기가 어떻게 돼요?

등록

싫다. 그거만큼

얘만큼 크기가 스케일이 되겠죠 그 행위를 모든 필터에 대해서 다 해주는 거예요.
그렇게 해서 그 축을 담당하는 베이시스 벡터를 만드는 겁니다.
그러고 나서 알파랑 메타를 바꿔가면서 로스를 그림을 그리면 이렇게 나오더라라는 거예요.
그렇게 해서 로스 레드 스킵을 그려봤고 스키 커넥션의 유무 혹은 레이어가 많아질수록 어떻게 되는지 그걸 본 거예요.
봤더니 노스키 커넥션 스키 커넥션 안 했더니 ns 로스 모양이 이상해지더라.
abc랑 be f를 비교하면 되겠죠 def부터 보겠습니다.
층을 깊게 만드니까 스키 커넥션 없이 깊게 만드니까 어떻게 됐습니까? 루스를 모양이

루스의 모양이 d의 두 순으로 루스의 모양이 찌그러져 찌그러지죠 반면에 abc는 어떻습니까? 예뻐요 이쁘죠 그러니까 누가 더 학습이 쉽겠어요 abc가 학습이 쉬워요 df가 더 쉬워요 abc abc가 훨씬 쉽죠 바로 그겁니다.
그걸 밝혀낸 논문이 바로 이 루스 랜드 스킨 논문이 이걸 가지고 트랜스포머도 그려볼 수 있고 여러 가지 그려볼 수 있겠죠 그런 녀석이 그런 하나의 툴이에요.
로스 랜드스케이블 그릴 수 있는 툴 그겁니다. 스키 커넥션이라는 존재가 저렇게 좋다는 거고 스키 이 뭔지 설명 들어갑니다.
레지 넷.


clovanote.naver.com



# Summary

이 설명은 고차원의 손실(로스) 풍경(랜드스케이프)을 시각화하는 방법에 대한 것입니다. 손실 풍경은 모델의 매개변수(웨이트)에 대한 손실 함수의 값을 나타내는 그래프로, 모델의 학습 과정에서 최적의 매개변수를 찾는 데 도움을 줍니다. 이 풍경은 일반적으로 매우 고차원이기 때문에 직접적으로 시각화하기 어렵습니다.

설명에 따르면, 랜덤 벡터를 사용하여 고차원 손실 풍경을 시각화하는 방법에 대해 이야기하고 있습니다. 이 방법은 다음과 같은 단계를 포함합니다:

1. **랜덤 벡터 선택**: 웨이트 벡터와 동일한 크기를 가진 두 개의 랜덤 벡터를 선택합니다.
2. **스케일링**: 선택된 랜덤 벡터를 사용하여 웨이트 공간에서의 방향을 정의하고, 이 방향들을 따라 웨이트를 스케일링하여 손실 값을 계산합니다.
3. **알파와 베타 조정**: 알파와 베타라는 두 매개변수를 사용하여 랜덤 벡터의 스케일을 조정하면서 손실 값을 계산합니다. 이렇게 하여 손실 풍경의 2차원 '슬라이스'를 생성할 수 있습니다.
4. **시각화**: 알파와 베타를 변화시키면서 계산된 손실 값들을 3차원 공간에 플롯하여 손실 풍경을 시각화합니다.

이 방법은 고차원의 손실 풍경을 이해하고, 특히 깊은 신경망에서 발생할 수 있는 최적화 문제들(예: 언더피팅, 오버피팅, 로컬 미니멈 문제 등)을 시각적으로 분석하는 데 유용합니다. 또한, 이 방법은 레즈넷(ResNet)과 같은 깊은 신경망의 스킵 커넥션(skip connection)이 손실 풍경에 어떤 영향을 미치는지를 보여주는 데 사용되었습니다. 스킵 커넥션은 손실 풍경을 더 '평평하게' 만들어 최적화를 용이하게 하는 것으로 알려져 있습니다.

## 

### Loss Landscape의 중요성

- **최적화**: Loss landscape는 최적화 과정에서 중요한 역할을 합니다. 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘은 이 표면을 따라 가장 낮은 지점(전역 최소값)을 찾으려고 시도합니다.
- **모델의 이해**: Loss landscape를 통해 모델의 학습 가능성과 복잡성을 이해할 수 있습니다. 예를 들어, 매우 복잡하고 깊은 모델은 매우 울퉁불퉁하고 복잡한 loss landscape를 가질 수 있습니다.
- **과적합(Overfitting)**: Loss landscape는 과적합을 이해하는 데에도 도움을 줄 수 있습니다. 훈련 데이터에 대해 너무 낮은 손실을 가지는 지점은 과적합을 나타낼 수 있습니다.
- **일반화(Generalization)**: 좋은 일반화를 가진 모델은 보통 더 부드러운 loss landscape를 가집니다. 이는 모델이 새로운 데이터에 대해 더 잘 작동할 가능성이 높음을 의미합니다.

### Loss Landscape의 특징

- **전역 최소값(Global Minimum)**: 손실 함수가 가장 낮은 지점입니다. 이론적으로 이 지점에서 모델은 최적의 성능을 발휘합니다.
- **지역 최소값(Local Minimum)**: 손실 함수가 주변보다 낮지만 전역 최소값은 아닌 지점입니다. 모델이 이러한 지점에 갇히면 더 이상 개선되지 않을 수 있습니다.
- **안장점(Saddle Point)**: 주변의 일부 방향에서는 최소값이지만 다른 방향에서는 최대값인 지점입니다. 고차원에서는 안장점이 흔하게 발생합니다.
- **평탄한 지역(Flat Regions)**: 손실이 거의 변하지 않는 지역입니다. 이러한 지역에서는 경사 하강법이 느려질 수 있습니다.

### 시각화

Loss landscape는 고차원이기 때문에 직접적으로 시각화하기 어렵습니다. 그러나 연구자들은 다양한 기법을 사용하여 2D 또는 3D로 표현합니다. 예를 들어, 두 개의 가중치를 선택하여 그에 대한 손실 값을 표면으로 그리거나, 주성분 분석(PCA) 또는 t-SNE와 같은 차원 축소 기법을 사용하여 시각화할 수 있습니다.

### 결론

Loss landscape는 모델의 학습 과정과 최적화를 이해하는 데 중요한 도구입니다. 그러나 실제로는 매우 고차원이고 복잡하기 때문에, 연구자들은 이를 분석하고 이해하기 위해 다양한 방법론을 개발하고 있습니다.
