digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2184884525232 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	2184884231552 [label=AddmmBackward0]
	2184884231408 -> 2184884231552
	2184884521392 [label="fc.bias
 (1000)" fillcolor=lightblue]
	2184884521392 -> 2184884231408
	2184884231408 [label=AccumulateGrad]
	2184884231360 -> 2184884231552
	2184884231360 [label=ViewBackward0]
	2184884231264 -> 2184884231360
	2184884231264 [label=MeanBackward1]
	2184884230976 -> 2184884231264
	2184884230976 [label=ReluBackward0]
	2184884230880 -> 2184884230976
	2184884230880 [label=AddBackward0]
	2184884230784 -> 2184884230880
	2184884230784 [label=NativeBatchNormBackward0]
	2184884230544 -> 2184884230784
	2184884230544 [label=ConvolutionBackward0]
	2184884230304 -> 2184884230544
	2184884230304 [label=ReluBackward0]
	2184884230112 -> 2184884230304
	2184884230112 [label=NativeBatchNormBackward0]
	2184884232224 -> 2184884230112
	2184884232224 [label=ConvolutionBackward0]
	2184884230832 -> 2184884232224
	2184884230832 [label=ReluBackward0]
	2184884232512 -> 2184884230832
	2184884232512 [label=AddBackward0]
	2184884232608 -> 2184884232512
	2184884232608 [label=NativeBatchNormBackward0]
	2184884232752 -> 2184884232608
	2184884232752 [label=ConvolutionBackward0]
	2184884232944 -> 2184884232752
	2184884232944 [label=ReluBackward0]
	2184884233088 -> 2184884232944
	2184884233088 [label=NativeBatchNormBackward0]
	2184884233184 -> 2184884233088
	2184884233184 [label=ConvolutionBackward0]
	2184884233376 -> 2184884233184
	2184884233376 [label=ReluBackward0]
	2184884233520 -> 2184884233376
	2184884233520 [label=AddBackward0]
	2184884233616 -> 2184884233520
	2184884233616 [label=NativeBatchNormBackward0]
	2184884233760 -> 2184884233616
	2184884233760 [label=ConvolutionBackward0]
	2184884233952 -> 2184884233760
	2184884233952 [label=ReluBackward0]
	2184884234096 -> 2184884233952
	2184884234096 [label=NativeBatchNormBackward0]
	2184884234192 -> 2184884234096
	2184884234192 [label=ConvolutionBackward0]
	2184884233568 -> 2184884234192
	2184884233568 [label=ReluBackward0]
	2184884234480 -> 2184884233568
	2184884234480 [label=AddBackward0]
	2184884234576 -> 2184884234480
	2184884234576 [label=NativeBatchNormBackward0]
	2184884234720 -> 2184884234576
	2184884234720 [label=ConvolutionBackward0]
	2184884234912 -> 2184884234720
	2184884234912 [label=ReluBackward0]
	2184884235056 -> 2184884234912
	2184884235056 [label=NativeBatchNormBackward0]
	2184884235152 -> 2184884235056
	2184884235152 [label=ConvolutionBackward0]
	2184884235344 -> 2184884235152
	2184884235344 [label=ReluBackward0]
	2184884235488 -> 2184884235344
	2184884235488 [label=AddBackward0]
	2184884235584 -> 2184884235488
	2184884235584 [label=NativeBatchNormBackward0]
	2184884235728 -> 2184884235584
	2184884235728 [label=ConvolutionBackward0]
	2184884235920 -> 2184884235728
	2184884235920 [label=ReluBackward0]
	2184884236064 -> 2184884235920
	2184884236064 [label=NativeBatchNormBackward0]
	2184884236160 -> 2184884236064
	2184884236160 [label=ConvolutionBackward0]
	2184884235536 -> 2184884236160
	2184884235536 [label=ReluBackward0]
	2184884236448 -> 2184884235536
	2184884236448 [label=AddBackward0]
	2184884236544 -> 2184884236448
	2184884236544 [label=NativeBatchNormBackward0]
	2184884236688 -> 2184884236544
	2184884236688 [label=ConvolutionBackward0]
	2184884236880 -> 2184884236688
	2184884236880 [label=ReluBackward0]
	2184884237024 -> 2184884236880
	2184884237024 [label=NativeBatchNormBackward0]
	2184884237120 -> 2184884237024
	2184884237120 [label=ConvolutionBackward0]
	2184884237312 -> 2184884237120
	2184884237312 [label=ReluBackward0]
	2184884237456 -> 2184884237312
	2184884237456 [label=AddBackward0]
	2184884237552 -> 2184884237456
	2184884237552 [label=NativeBatchNormBackward0]
	2184884237696 -> 2184884237552
	2184884237696 [label=ConvolutionBackward0]
	2184884237888 -> 2184884237696
	2184884237888 [label=ReluBackward0]
	2184884238032 -> 2184884237888
	2184884238032 [label=NativeBatchNormBackward0]
	2184884238128 -> 2184884238032
	2184884238128 [label=ConvolutionBackward0]
	2184884237504 -> 2184884238128
	2184884237504 [label=ReluBackward0]
	2184884238416 -> 2184884237504
	2184884238416 [label=AddBackward0]
	2184884238512 -> 2184884238416
	2184884238512 [label=NativeBatchNormBackward0]
	2184884238656 -> 2184884238512
	2184884238656 [label=ConvolutionBackward0]
	2184884238848 -> 2184884238656
	2184884238848 [label=ReluBackward0]
	2184884238992 -> 2184884238848
	2184884238992 [label=NativeBatchNormBackward0]
	2184884239088 -> 2184884238992
	2184884239088 [label=ConvolutionBackward0]
	2184884238464 -> 2184884239088
	2184884238464 [label=MaxPool2DWithIndicesBackward0]
	2184884239376 -> 2184884238464
	2184884239376 [label=ReluBackward0]
	2184884239472 -> 2184884239376
	2184884239472 [label=NativeBatchNormBackward0]
	2184884239568 -> 2184884239472
	2184884239568 [label=ConvolutionBackward0]
	2184884239760 -> 2184884239568
	2184873442768 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2184873442768 -> 2184884239760
	2184884239760 [label=AccumulateGrad]
	2184884239520 -> 2184884239472
	2184873439696 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2184873439696 -> 2184884239520
	2184884239520 [label=AccumulateGrad]
	2184884239184 -> 2184884239472
	2184851639760 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2184851639760 -> 2184884239184
	2184884239184 [label=AccumulateGrad]
	2184884239280 -> 2184884239088
	2184884100784 [label="layer1.0.residual.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2184884100784 -> 2184884239280
	2184884239280 [label=AccumulateGrad]
	2184884239040 -> 2184884238992
	2184884100880 [label="layer1.0.residual.1.weight
 (64)" fillcolor=lightblue]
	2184884100880 -> 2184884239040
	2184884239040 [label=AccumulateGrad]
	2184884238896 -> 2184884238992
	2184884100976 [label="layer1.0.residual.1.bias
 (64)" fillcolor=lightblue]
	2184884100976 -> 2184884238896
	2184884238896 [label=AccumulateGrad]
	2184884238800 -> 2184884238656
	2184884101360 [label="layer1.0.residual.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2184884101360 -> 2184884238800
	2184884238800 [label=AccumulateGrad]
	2184884238608 -> 2184884238512
	2184884101456 [label="layer1.0.residual.4.weight
 (64)" fillcolor=lightblue]
	2184884101456 -> 2184884238608
	2184884238608 [label=AccumulateGrad]
	2184884238560 -> 2184884238512
	2184884101552 [label="layer1.0.residual.4.bias
 (64)" fillcolor=lightblue]
	2184884101552 -> 2184884238560
	2184884238560 [label=AccumulateGrad]
	2184884238464 -> 2184884238416
	2184884238320 -> 2184884238128
	2184884101936 [label="layer1.1.residual.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2184884101936 -> 2184884238320
	2184884238320 [label=AccumulateGrad]
	2184884238080 -> 2184884238032
	2184884102032 [label="layer1.1.residual.1.weight
 (64)" fillcolor=lightblue]
	2184884102032 -> 2184884238080
	2184884238080 [label=AccumulateGrad]
	2184884237936 -> 2184884238032
	2184884102128 [label="layer1.1.residual.1.bias
 (64)" fillcolor=lightblue]
	2184884102128 -> 2184884237936
	2184884237936 [label=AccumulateGrad]
	2184884237840 -> 2184884237696
	2184884102512 [label="layer1.1.residual.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2184884102512 -> 2184884237840
	2184884237840 [label=AccumulateGrad]
	2184884237648 -> 2184884237552
	2184884102608 [label="layer1.1.residual.4.weight
 (64)" fillcolor=lightblue]
	2184884102608 -> 2184884237648
	2184884237648 [label=AccumulateGrad]
	2184884237600 -> 2184884237552
	2184884102704 [label="layer1.1.residual.4.bias
 (64)" fillcolor=lightblue]
	2184884102704 -> 2184884237600
	2184884237600 [label=AccumulateGrad]
	2184884237504 -> 2184884237456
	2184884237264 -> 2184884237120
	2184884103664 [label="layer2.0.residual.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2184884103664 -> 2184884237264
	2184884237264 [label=AccumulateGrad]
	2184884237072 -> 2184884237024
	2184884103760 [label="layer2.0.residual.1.weight
 (128)" fillcolor=lightblue]
	2184884103760 -> 2184884237072
	2184884237072 [label=AccumulateGrad]
	2184884236928 -> 2184884237024
	2184884103856 [label="layer2.0.residual.1.bias
 (128)" fillcolor=lightblue]
	2184884103856 -> 2184884236928
	2184884236928 [label=AccumulateGrad]
	2184884236832 -> 2184884236688
	2184884104240 [label="layer2.0.residual.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2184884104240 -> 2184884236832
	2184884236832 [label=AccumulateGrad]
	2184884236640 -> 2184884236544
	2184884104336 [label="layer2.0.residual.4.weight
 (128)" fillcolor=lightblue]
	2184884104336 -> 2184884236640
	2184884236640 [label=AccumulateGrad]
	2184884236592 -> 2184884236544
	2184884104432 [label="layer2.0.residual.4.bias
 (128)" fillcolor=lightblue]
	2184884104432 -> 2184884236592
	2184884236592 [label=AccumulateGrad]
	2184884236496 -> 2184884236448
	2184884236496 [label=NativeBatchNormBackward0]
	2184884237216 -> 2184884236496
	2184884237216 [label=ConvolutionBackward0]
	2184884237312 -> 2184884237216
	2184884237360 -> 2184884237216
	2184884103088 [label="layer2.0.projection.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2184884103088 -> 2184884237360
	2184884237360 [label=AccumulateGrad]
	2184884236784 -> 2184884236496
	2184884103184 [label="layer2.0.projection.1.weight
 (128)" fillcolor=lightblue]
	2184884103184 -> 2184884236784
	2184884236784 [label=AccumulateGrad]
	2184884236736 -> 2184884236496
	2184884103280 [label="layer2.0.projection.1.bias
 (128)" fillcolor=lightblue]
	2184884103280 -> 2184884236736
	2184884236736 [label=AccumulateGrad]
	2184884236352 -> 2184884236160
	2184884104816 [label="layer2.1.residual.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2184884104816 -> 2184884236352
	2184884236352 [label=AccumulateGrad]
	2184884236112 -> 2184884236064
	2184884104912 [label="layer2.1.residual.1.weight
 (128)" fillcolor=lightblue]
	2184884104912 -> 2184884236112
	2184884236112 [label=AccumulateGrad]
	2184884235968 -> 2184884236064
	2184884105008 [label="layer2.1.residual.1.bias
 (128)" fillcolor=lightblue]
	2184884105008 -> 2184884235968
	2184884235968 [label=AccumulateGrad]
	2184884235872 -> 2184884235728
	2184884105392 [label="layer2.1.residual.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2184884105392 -> 2184884235872
	2184884235872 [label=AccumulateGrad]
	2184884235680 -> 2184884235584
	2184884105488 [label="layer2.1.residual.4.weight
 (128)" fillcolor=lightblue]
	2184884105488 -> 2184884235680
	2184884235680 [label=AccumulateGrad]
	2184884235632 -> 2184884235584
	2184884105584 [label="layer2.1.residual.4.bias
 (128)" fillcolor=lightblue]
	2184884105584 -> 2184884235632
	2184884235632 [label=AccumulateGrad]
	2184884235536 -> 2184884235488
	2184884235296 -> 2184884235152
	2184884106544 [label="layer3.0.residual.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2184884106544 -> 2184884235296
	2184884235296 [label=AccumulateGrad]
	2184884235104 -> 2184884235056
	2184884106640 [label="layer3.0.residual.1.weight
 (256)" fillcolor=lightblue]
	2184884106640 -> 2184884235104
	2184884235104 [label=AccumulateGrad]
	2184884234960 -> 2184884235056
	2184884106736 [label="layer3.0.residual.1.bias
 (256)" fillcolor=lightblue]
	2184884106736 -> 2184884234960
	2184884234960 [label=AccumulateGrad]
	2184884234864 -> 2184884234720
	2184884107120 [label="layer3.0.residual.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184884107120 -> 2184884234864
	2184884234864 [label=AccumulateGrad]
	2184884234672 -> 2184884234576
	2184884107216 [label="layer3.0.residual.4.weight
 (256)" fillcolor=lightblue]
	2184884107216 -> 2184884234672
	2184884234672 [label=AccumulateGrad]
	2184884234624 -> 2184884234576
	2184884107312 [label="layer3.0.residual.4.bias
 (256)" fillcolor=lightblue]
	2184884107312 -> 2184884234624
	2184884234624 [label=AccumulateGrad]
	2184884234528 -> 2184884234480
	2184884234528 [label=NativeBatchNormBackward0]
	2184884235248 -> 2184884234528
	2184884235248 [label=ConvolutionBackward0]
	2184884235344 -> 2184884235248
	2184884235392 -> 2184884235248
	2184884105968 [label="layer3.0.projection.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2184884105968 -> 2184884235392
	2184884235392 [label=AccumulateGrad]
	2184884234816 -> 2184884234528
	2184884106064 [label="layer3.0.projection.1.weight
 (256)" fillcolor=lightblue]
	2184884106064 -> 2184884234816
	2184884234816 [label=AccumulateGrad]
	2184884234768 -> 2184884234528
	2184884106160 [label="layer3.0.projection.1.bias
 (256)" fillcolor=lightblue]
	2184884106160 -> 2184884234768
	2184884234768 [label=AccumulateGrad]
	2184884234384 -> 2184884234192
	2184884107696 [label="layer3.1.residual.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184884107696 -> 2184884234384
	2184884234384 [label=AccumulateGrad]
	2184884234144 -> 2184884234096
	2184884107792 [label="layer3.1.residual.1.weight
 (256)" fillcolor=lightblue]
	2184884107792 -> 2184884234144
	2184884234144 [label=AccumulateGrad]
	2184884234000 -> 2184884234096
	2184884107888 [label="layer3.1.residual.1.bias
 (256)" fillcolor=lightblue]
	2184884107888 -> 2184884234000
	2184884234000 [label=AccumulateGrad]
	2184884233904 -> 2184884233760
	2184884108272 [label="layer3.1.residual.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2184884108272 -> 2184884233904
	2184884233904 [label=AccumulateGrad]
	2184884233712 -> 2184884233616
	2184884108368 [label="layer3.1.residual.4.weight
 (256)" fillcolor=lightblue]
	2184884108368 -> 2184884233712
	2184884233712 [label=AccumulateGrad]
	2184884233664 -> 2184884233616
	2184884108464 [label="layer3.1.residual.4.bias
 (256)" fillcolor=lightblue]
	2184884108464 -> 2184884233664
	2184884233664 [label=AccumulateGrad]
	2184884233568 -> 2184884233520
	2184884233328 -> 2184884233184
	2184884519088 [label="layer4.0.residual.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2184884519088 -> 2184884233328
	2184884233328 [label=AccumulateGrad]
	2184884233136 -> 2184884233088
	2184884519184 [label="layer4.0.residual.1.weight
 (512)" fillcolor=lightblue]
	2184884519184 -> 2184884233136
	2184884233136 [label=AccumulateGrad]
	2184884232992 -> 2184884233088
	2184884519280 [label="layer4.0.residual.1.bias
 (512)" fillcolor=lightblue]
	2184884519280 -> 2184884232992
	2184884232992 [label=AccumulateGrad]
	2184884232896 -> 2184884232752
	2184884519664 [label="layer4.0.residual.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2184884519664 -> 2184884232896
	2184884232896 [label=AccumulateGrad]
	2184884232704 -> 2184884232608
	2184884519760 [label="layer4.0.residual.4.weight
 (512)" fillcolor=lightblue]
	2184884519760 -> 2184884232704
	2184884232704 [label=AccumulateGrad]
	2184884232656 -> 2184884232608
	2184884519856 [label="layer4.0.residual.4.bias
 (512)" fillcolor=lightblue]
	2184884519856 -> 2184884232656
	2184884232656 [label=AccumulateGrad]
	2184884232560 -> 2184884232512
	2184884232560 [label=NativeBatchNormBackward0]
	2184884233280 -> 2184884232560
	2184884233280 [label=ConvolutionBackward0]
	2184884233376 -> 2184884233280
	2184884233424 -> 2184884233280
	2184884108848 [label="layer4.0.projection.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2184884108848 -> 2184884233424
	2184884233424 [label=AccumulateGrad]
	2184884232848 -> 2184884232560
	2184884108944 [label="layer4.0.projection.1.weight
 (512)" fillcolor=lightblue]
	2184884108944 -> 2184884232848
	2184884232848 [label=AccumulateGrad]
	2184884232800 -> 2184884232560
	2184884109040 [label="layer4.0.projection.1.bias
 (512)" fillcolor=lightblue]
	2184884109040 -> 2184884232800
	2184884232800 [label=AccumulateGrad]
	2184884232416 -> 2184884232224
	2184884520240 [label="layer4.1.residual.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2184884520240 -> 2184884232416
	2184884232416 [label=AccumulateGrad]
	2184884232176 -> 2184884230112
	2184884520336 [label="layer4.1.residual.1.weight
 (512)" fillcolor=lightblue]
	2184884520336 -> 2184884232176
	2184884232176 [label=AccumulateGrad]
	2184884230256 -> 2184884230112
	2184884520432 [label="layer4.1.residual.1.bias
 (512)" fillcolor=lightblue]
	2184884520432 -> 2184884230256
	2184884230256 [label=AccumulateGrad]
	2184884230400 -> 2184884230544
	2184884520816 [label="layer4.1.residual.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2184884520816 -> 2184884230400
	2184884230400 [label=AccumulateGrad]
	2184884230688 -> 2184884230784
	2184884520912 [label="layer4.1.residual.4.weight
 (512)" fillcolor=lightblue]
	2184884520912 -> 2184884230688
	2184884230688 [label=AccumulateGrad]
	2184884230736 -> 2184884230784
	2184884521008 [label="layer4.1.residual.4.bias
 (512)" fillcolor=lightblue]
	2184884521008 -> 2184884230736
	2184884230736 [label=AccumulateGrad]
	2184884230832 -> 2184884230880
	2184884231312 -> 2184884231552
	2184884231312 [label=TBackward0]
	2184884230928 -> 2184884231312
	2184884521296 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	2184884521296 -> 2184884230928
	2184884230928 [label=AccumulateGrad]
	2184884231552 -> 2184884525232
}
