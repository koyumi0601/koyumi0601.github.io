digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2722076218128 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	2722075891632 [label=AddmmBackward0]
	2722075891488 -> 2722075891632
	2722076214192 [label="fc.bias
 (1000)" fillcolor=lightblue]
	2722076214192 -> 2722075891488
	2722075891488 [label=AccumulateGrad]
	2722075891440 -> 2722075891632
	2722075891440 [label=ViewBackward0]
	2722075891344 -> 2722075891440
	2722075891344 [label=MeanBackward1]
	2722075891056 -> 2722075891344
	2722075891056 [label=ReluBackward0]
	2722075890960 -> 2722075891056
	2722075890960 [label=AddBackward0]
	2722075890864 -> 2722075890960
	2722075890864 [label=NativeBatchNormBackward0]
	2722075890624 -> 2722075890864
	2722075890624 [label=ConvolutionBackward0]
	2722075890384 -> 2722075890624
	2722075890384 [label=ReluBackward0]
	2722075890192 -> 2722075890384
	2722075890192 [label=NativeBatchNormBackward0]
	2722075892304 -> 2722075890192
	2722075892304 [label=ConvolutionBackward0]
	2722075890912 -> 2722075892304
	2722075890912 [label=ReluBackward0]
	2722075892592 -> 2722075890912
	2722075892592 [label=AddBackward0]
	2722075892688 -> 2722075892592
	2722075892688 [label=NativeBatchNormBackward0]
	2722075892832 -> 2722075892688
	2722075892832 [label=ConvolutionBackward0]
	2722075893024 -> 2722075892832
	2722075893024 [label=ReluBackward0]
	2722075893168 -> 2722075893024
	2722075893168 [label=NativeBatchNormBackward0]
	2722075893264 -> 2722075893168
	2722075893264 [label=ConvolutionBackward0]
	2722075893456 -> 2722075893264
	2722075893456 [label=ReluBackward0]
	2722075893600 -> 2722075893456
	2722075893600 [label=AddBackward0]
	2722075893696 -> 2722075893600
	2722075893696 [label=NativeBatchNormBackward0]
	2722075893840 -> 2722075893696
	2722075893840 [label=ConvolutionBackward0]
	2722075894032 -> 2722075893840
	2722075894032 [label=ReluBackward0]
	2722075894176 -> 2722075894032
	2722075894176 [label=NativeBatchNormBackward0]
	2722075894272 -> 2722075894176
	2722075894272 [label=ConvolutionBackward0]
	2722075893648 -> 2722075894272
	2722075893648 [label=ReluBackward0]
	2722075894560 -> 2722075893648
	2722075894560 [label=AddBackward0]
	2722075894656 -> 2722075894560
	2722075894656 [label=NativeBatchNormBackward0]
	2722075894800 -> 2722075894656
	2722075894800 [label=ConvolutionBackward0]
	2722075894992 -> 2722075894800
	2722075894992 [label=ReluBackward0]
	2722075895136 -> 2722075894992
	2722075895136 [label=NativeBatchNormBackward0]
	2722075895232 -> 2722075895136
	2722075895232 [label=ConvolutionBackward0]
	2722075895424 -> 2722075895232
	2722075895424 [label=ReluBackward0]
	2722075895568 -> 2722075895424
	2722075895568 [label=AddBackward0]
	2722075895664 -> 2722075895568
	2722075895664 [label=NativeBatchNormBackward0]
	2722075895808 -> 2722075895664
	2722075895808 [label=ConvolutionBackward0]
	2722075896000 -> 2722075895808
	2722075896000 [label=ReluBackward0]
	2722075896144 -> 2722075896000
	2722075896144 [label=NativeBatchNormBackward0]
	2722075896240 -> 2722075896144
	2722075896240 [label=ConvolutionBackward0]
	2722075895616 -> 2722075896240
	2722075895616 [label=ReluBackward0]
	2722075896528 -> 2722075895616
	2722075896528 [label=AddBackward0]
	2722075896624 -> 2722075896528
	2722075896624 [label=NativeBatchNormBackward0]
	2722075896768 -> 2722075896624
	2722075896768 [label=ConvolutionBackward0]
	2722075896960 -> 2722075896768
	2722075896960 [label=ReluBackward0]
	2722075897104 -> 2722075896960
	2722075897104 [label=NativeBatchNormBackward0]
	2722075897200 -> 2722075897104
	2722075897200 [label=ConvolutionBackward0]
	2722075897392 -> 2722075897200
	2722075897392 [label=ReluBackward0]
	2722075897536 -> 2722075897392
	2722075897536 [label=AddBackward0]
	2722075897632 -> 2722075897536
	2722075897632 [label=NativeBatchNormBackward0]
	2722075897776 -> 2722075897632
	2722075897776 [label=ConvolutionBackward0]
	2722075897968 -> 2722075897776
	2722075897968 [label=ReluBackward0]
	2722075898112 -> 2722075897968
	2722075898112 [label=NativeBatchNormBackward0]
	2722075898208 -> 2722075898112
	2722075898208 [label=ConvolutionBackward0]
	2722075897584 -> 2722075898208
	2722075897584 [label=ReluBackward0]
	2722075898496 -> 2722075897584
	2722075898496 [label=AddBackward0]
	2722075898592 -> 2722075898496
	2722075898592 [label=NativeBatchNormBackward0]
	2722075898736 -> 2722075898592
	2722075898736 [label=ConvolutionBackward0]
	2722075898928 -> 2722075898736
	2722075898928 [label=ReluBackward0]
	2722075899072 -> 2722075898928
	2722075899072 [label=NativeBatchNormBackward0]
	2722075899168 -> 2722075899072
	2722075899168 [label=ConvolutionBackward0]
	2722075898544 -> 2722075899168
	2722075898544 [label=MaxPool2DWithIndicesBackward0]
	2722075899456 -> 2722075898544
	2722075899456 [label=ReluBackward0]
	2722075899552 -> 2722075899456
	2722075899552 [label=NativeBatchNormBackward0]
	2722075899648 -> 2722075899552
	2722075899648 [label=ConvolutionBackward0]
	2722075899840 -> 2722075899648
	2722066031888 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2722066031888 -> 2722075899840
	2722075899840 [label=AccumulateGrad]
	2722075899600 -> 2722075899552
	2722075777200 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2722075777200 -> 2722075899600
	2722075899600 [label=AccumulateGrad]
	2722075899264 -> 2722075899552
	2722075777296 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2722075777296 -> 2722075899264
	2722075899264 [label=AccumulateGrad]
	2722075899360 -> 2722075899168
	2722065900432 [label="layer1.0.residual.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2722065900432 -> 2722075899360
	2722075899360 [label=AccumulateGrad]
	2722075899120 -> 2722075899072
	2722043839504 [label="layer1.0.residual.1.weight
 (64)" fillcolor=lightblue]
	2722043839504 -> 2722075899120
	2722075899120 [label=AccumulateGrad]
	2722075898976 -> 2722075899072
	2722066028144 [label="layer1.0.residual.1.bias
 (64)" fillcolor=lightblue]
	2722066028144 -> 2722075898976
	2722075898976 [label=AccumulateGrad]
	2722075898880 -> 2722075898736
	2722075777872 [label="layer1.0.residual.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2722075777872 -> 2722075898880
	2722075898880 [label=AccumulateGrad]
	2722075898688 -> 2722075898592
	2722075777968 [label="layer1.0.residual.4.weight
 (64)" fillcolor=lightblue]
	2722075777968 -> 2722075898688
	2722075898688 [label=AccumulateGrad]
	2722075898640 -> 2722075898592
	2722075778064 [label="layer1.0.residual.4.bias
 (64)" fillcolor=lightblue]
	2722075778064 -> 2722075898640
	2722075898640 [label=AccumulateGrad]
	2722075898544 -> 2722075898496
	2722075898400 -> 2722075898208
	2722075778448 [label="layer1.1.residual.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2722075778448 -> 2722075898400
	2722075898400 [label=AccumulateGrad]
	2722075898160 -> 2722075898112
	2722075778544 [label="layer1.1.residual.1.weight
 (64)" fillcolor=lightblue]
	2722075778544 -> 2722075898160
	2722075898160 [label=AccumulateGrad]
	2722075898016 -> 2722075898112
	2722075778640 [label="layer1.1.residual.1.bias
 (64)" fillcolor=lightblue]
	2722075778640 -> 2722075898016
	2722075898016 [label=AccumulateGrad]
	2722075897920 -> 2722075897776
	2722075779024 [label="layer1.1.residual.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2722075779024 -> 2722075897920
	2722075897920 [label=AccumulateGrad]
	2722075897728 -> 2722075897632
	2722075779120 [label="layer1.1.residual.4.weight
 (64)" fillcolor=lightblue]
	2722075779120 -> 2722075897728
	2722075897728 [label=AccumulateGrad]
	2722075897680 -> 2722075897632
	2722075779216 [label="layer1.1.residual.4.bias
 (64)" fillcolor=lightblue]
	2722075779216 -> 2722075897680
	2722075897680 [label=AccumulateGrad]
	2722075897584 -> 2722075897536
	2722075897344 -> 2722075897200
	2722075780176 [label="layer2.0.residual.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2722075780176 -> 2722075897344
	2722075897344 [label=AccumulateGrad]
	2722075897152 -> 2722075897104
	2722075780272 [label="layer2.0.residual.1.weight
 (128)" fillcolor=lightblue]
	2722075780272 -> 2722075897152
	2722075897152 [label=AccumulateGrad]
	2722075897008 -> 2722075897104
	2722075780368 [label="layer2.0.residual.1.bias
 (128)" fillcolor=lightblue]
	2722075780368 -> 2722075897008
	2722075897008 [label=AccumulateGrad]
	2722075896912 -> 2722075896768
	2722075780752 [label="layer2.0.residual.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2722075780752 -> 2722075896912
	2722075896912 [label=AccumulateGrad]
	2722075896720 -> 2722075896624
	2722075780848 [label="layer2.0.residual.4.weight
 (128)" fillcolor=lightblue]
	2722075780848 -> 2722075896720
	2722075896720 [label=AccumulateGrad]
	2722075896672 -> 2722075896624
	2722075780944 [label="layer2.0.residual.4.bias
 (128)" fillcolor=lightblue]
	2722075780944 -> 2722075896672
	2722075896672 [label=AccumulateGrad]
	2722075896576 -> 2722075896528
	2722075896576 [label=NativeBatchNormBackward0]
	2722075897296 -> 2722075896576
	2722075897296 [label=ConvolutionBackward0]
	2722075897392 -> 2722075897296
	2722075897440 -> 2722075897296
	2722075779600 [label="layer2.0.projection.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2722075779600 -> 2722075897440
	2722075897440 [label=AccumulateGrad]
	2722075896864 -> 2722075896576
	2722075779696 [label="layer2.0.projection.1.weight
 (128)" fillcolor=lightblue]
	2722075779696 -> 2722075896864
	2722075896864 [label=AccumulateGrad]
	2722075896816 -> 2722075896576
	2722075779792 [label="layer2.0.projection.1.bias
 (128)" fillcolor=lightblue]
	2722075779792 -> 2722075896816
	2722075896816 [label=AccumulateGrad]
	2722075896432 -> 2722075896240
	2722075781328 [label="layer2.1.residual.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2722075781328 -> 2722075896432
	2722075896432 [label=AccumulateGrad]
	2722075896192 -> 2722075896144
	2722075781424 [label="layer2.1.residual.1.weight
 (128)" fillcolor=lightblue]
	2722075781424 -> 2722075896192
	2722075896192 [label=AccumulateGrad]
	2722075896048 -> 2722075896144
	2722075781520 [label="layer2.1.residual.1.bias
 (128)" fillcolor=lightblue]
	2722075781520 -> 2722075896048
	2722075896048 [label=AccumulateGrad]
	2722075895952 -> 2722075895808
	2722075781904 [label="layer2.1.residual.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2722075781904 -> 2722075895952
	2722075895952 [label=AccumulateGrad]
	2722075895760 -> 2722075895664
	2722075782000 [label="layer2.1.residual.4.weight
 (128)" fillcolor=lightblue]
	2722075782000 -> 2722075895760
	2722075895760 [label=AccumulateGrad]
	2722075895712 -> 2722075895664
	2722075782096 [label="layer2.1.residual.4.bias
 (128)" fillcolor=lightblue]
	2722075782096 -> 2722075895712
	2722075895712 [label=AccumulateGrad]
	2722075895616 -> 2722075895568
	2722075895376 -> 2722075895232
	2722075783056 [label="layer3.0.residual.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2722075783056 -> 2722075895376
	2722075895376 [label=AccumulateGrad]
	2722075895184 -> 2722075895136
	2722075783152 [label="layer3.0.residual.1.weight
 (256)" fillcolor=lightblue]
	2722075783152 -> 2722075895184
	2722075895184 [label=AccumulateGrad]
	2722075895040 -> 2722075895136
	2722075783248 [label="layer3.0.residual.1.bias
 (256)" fillcolor=lightblue]
	2722075783248 -> 2722075895040
	2722075895040 [label=AccumulateGrad]
	2722075894944 -> 2722075894800
	2722075783632 [label="layer3.0.residual.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2722075783632 -> 2722075894944
	2722075894944 [label=AccumulateGrad]
	2722075894752 -> 2722075894656
	2722075783728 [label="layer3.0.residual.4.weight
 (256)" fillcolor=lightblue]
	2722075783728 -> 2722075894752
	2722075894752 [label=AccumulateGrad]
	2722075894704 -> 2722075894656
	2722075783824 [label="layer3.0.residual.4.bias
 (256)" fillcolor=lightblue]
	2722075783824 -> 2722075894704
	2722075894704 [label=AccumulateGrad]
	2722075894608 -> 2722075894560
	2722075894608 [label=NativeBatchNormBackward0]
	2722075895328 -> 2722075894608
	2722075895328 [label=ConvolutionBackward0]
	2722075895424 -> 2722075895328
	2722075895472 -> 2722075895328
	2722075782480 [label="layer3.0.projection.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2722075782480 -> 2722075895472
	2722075895472 [label=AccumulateGrad]
	2722075894896 -> 2722075894608
	2722075782576 [label="layer3.0.projection.1.weight
 (256)" fillcolor=lightblue]
	2722075782576 -> 2722075894896
	2722075894896 [label=AccumulateGrad]
	2722075894848 -> 2722075894608
	2722075782672 [label="layer3.0.projection.1.bias
 (256)" fillcolor=lightblue]
	2722075782672 -> 2722075894848
	2722075894848 [label=AccumulateGrad]
	2722075894464 -> 2722075894272
	2722075784208 [label="layer3.1.residual.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2722075784208 -> 2722075894464
	2722075894464 [label=AccumulateGrad]
	2722075894224 -> 2722075894176
	2722075784304 [label="layer3.1.residual.1.weight
 (256)" fillcolor=lightblue]
	2722075784304 -> 2722075894224
	2722075894224 [label=AccumulateGrad]
	2722075894080 -> 2722075894176
	2722075784400 [label="layer3.1.residual.1.bias
 (256)" fillcolor=lightblue]
	2722075784400 -> 2722075894080
	2722075894080 [label=AccumulateGrad]
	2722075893984 -> 2722075893840
	2722075784784 [label="layer3.1.residual.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2722075784784 -> 2722075893984
	2722075893984 [label=AccumulateGrad]
	2722075893792 -> 2722075893696
	2722075784880 [label="layer3.1.residual.4.weight
 (256)" fillcolor=lightblue]
	2722075784880 -> 2722075893792
	2722075893792 [label=AccumulateGrad]
	2722075893744 -> 2722075893696
	2722075784976 [label="layer3.1.residual.4.bias
 (256)" fillcolor=lightblue]
	2722075784976 -> 2722075893744
	2722075893744 [label=AccumulateGrad]
	2722075893648 -> 2722075893600
	2722075893408 -> 2722075893264
	2722075785936 [label="layer4.0.residual.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2722075785936 -> 2722075893408
	2722075893408 [label=AccumulateGrad]
	2722075893216 -> 2722075893168
	2722075786032 [label="layer4.0.residual.1.weight
 (512)" fillcolor=lightblue]
	2722075786032 -> 2722075893216
	2722075893216 [label=AccumulateGrad]
	2722075893072 -> 2722075893168
	2722075786128 [label="layer4.0.residual.1.bias
 (512)" fillcolor=lightblue]
	2722075786128 -> 2722075893072
	2722075893072 [label=AccumulateGrad]
	2722075892976 -> 2722075892832
	2722075786512 [label="layer4.0.residual.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2722075786512 -> 2722075892976
	2722075892976 [label=AccumulateGrad]
	2722075892784 -> 2722075892688
	2722075786608 [label="layer4.0.residual.4.weight
 (512)" fillcolor=lightblue]
	2722075786608 -> 2722075892784
	2722075892784 [label=AccumulateGrad]
	2722075892736 -> 2722075892688
	2722075786704 [label="layer4.0.residual.4.bias
 (512)" fillcolor=lightblue]
	2722075786704 -> 2722075892736
	2722075892736 [label=AccumulateGrad]
	2722075892640 -> 2722075892592
	2722075892640 [label=NativeBatchNormBackward0]
	2722075893360 -> 2722075892640
	2722075893360 [label=ConvolutionBackward0]
	2722075893456 -> 2722075893360
	2722075893504 -> 2722075893360
	2722075785360 [label="layer4.0.projection.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2722075785360 -> 2722075893504
	2722075893504 [label=AccumulateGrad]
	2722075892928 -> 2722075892640
	2722075785456 [label="layer4.0.projection.1.weight
 (512)" fillcolor=lightblue]
	2722075785456 -> 2722075892928
	2722075892928 [label=AccumulateGrad]
	2722075892880 -> 2722075892640
	2722075785552 [label="layer4.0.projection.1.bias
 (512)" fillcolor=lightblue]
	2722075785552 -> 2722075892880
	2722075892880 [label=AccumulateGrad]
	2722075892496 -> 2722075892304
	2722075787088 [label="layer4.1.residual.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2722075787088 -> 2722075892496
	2722075892496 [label=AccumulateGrad]
	2722075892256 -> 2722075890192
	2722075787184 [label="layer4.1.residual.1.weight
 (512)" fillcolor=lightblue]
	2722075787184 -> 2722075892256
	2722075892256 [label=AccumulateGrad]
	2722075890336 -> 2722075890192
	2722076213328 [label="layer4.1.residual.1.bias
 (512)" fillcolor=lightblue]
	2722076213328 -> 2722075890336
	2722075890336 [label=AccumulateGrad]
	2722075890480 -> 2722075890624
	2722076213712 [label="layer4.1.residual.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2722076213712 -> 2722075890480
	2722075890480 [label=AccumulateGrad]
	2722075890768 -> 2722075890864
	2722076213808 [label="layer4.1.residual.4.weight
 (512)" fillcolor=lightblue]
	2722076213808 -> 2722075890768
	2722075890768 [label=AccumulateGrad]
	2722075890816 -> 2722075890864
	2722076213904 [label="layer4.1.residual.4.bias
 (512)" fillcolor=lightblue]
	2722076213904 -> 2722075890816
	2722075890816 [label=AccumulateGrad]
	2722075890912 -> 2722075890960
	2722075891392 -> 2722075891632
	2722075891392 [label=TBackward0]
	2722075891008 -> 2722075891392
	2722076214288 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	2722076214288 -> 2722075891008
	2722075891008 [label=AccumulateGrad]
	2722075891632 -> 2722076218128
}
