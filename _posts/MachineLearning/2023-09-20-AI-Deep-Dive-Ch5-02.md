---
layout: single
title: "AI Deep Dive, Chapter 5. 이진 분류와 다중 분류 02. Sigmoid를 이용한 이진분류"
categories: machinelearning
tags: [ML, Machine Learning, AI, AI Deep Dive]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true


---

*AI Deep Dive Note*



# Chapter 5 - 02. Sigmoid를 이용한 이진분류

- 입력을 3x100x100인 (강아지 or 고양이) 사진이라고 하고, 곧바로 출력 층으로 연결한다고 하자.

- activation을 sigmoid를 사용한다의 의미
  - logistic regression을 수행한다.
-  분류 classification도 사실은 regression이다!
  - classification="logit을 regression하면서 풀었다"라고 말함.



## Logit 

- 이길 확률: q

- 질 확률: 1-q

- 승산 odds: $$ \frac{이길 확률 q}{질 확률 1-q} $$ 비율
- Logit log(odds): $$ log(\frac{q}{1-q})$$ 



## Logistic Regression

- Logit을 linear regression 한 것
- Regression: 회귀분석
- Linear regression: 선형회귀
  - 입력 변수와 출력 변수간의 선형 관계를 모델링
  - 수식: $$y = w_1 x_1 + w_2 x_2 + ... $$.
  - 출력 변수의 범위는 -무한대~+무한대 사이값
- Logistic regression: 로지스틱 회귀
  - 이진 출력 변수의 범위는 0~1 사이값
  - 확률을 의미한다.
  - 수식: $$ \sigma (z) = \frac{1}{1+e^{-z}} = \frac{1}{1+e^{-(w_1 x_1 + w_2 x_2 + ... )}}$$
    - linear regression의 출력을 sigmoid의 입력으로 넣은 것. 최종 출력을 sigmoid로 변환한다고 볼 수 있다.
  - Logit을 linear regression한 것? 
    - 로지스틱 회귀의 내부 작동 방식을 설명한 것
    - 로지스틱 회귀에서는 선형 회귀의 결과값을 로지스틱 함수 또는 시그모이드 함수에 통과시켜 0과 1사이의 확률 값을 얻는다.
    - 이렇게 해서, 로지스틱 회귀는 선형 회귀의 결과를 기반으로 이진 분류 문제를 해결.

![ch0502_1]({{site.url}}/images/$(filename)/ch0502_1.png)



## Loss 함수의 정의

- 머신의 출력 q = 강아지일 확률: 강아지(1), 고양이(0)
- 정답 y
- 워딩
  - "q=1 나오게 하자" = "q를 키우자"
  - "q=0 나오게 하자" = "1-q를 키우자"

- Loss 함수
  - 확률을 최대화한다<<로부터 loss함수를 정의할 수 있다.
  - 첫 번째 사진에 대해 머신이 예측한 해당 동물일 확률
    - (4줄짜리 표현) if 강아지, q를 maximize, else if 고양이, 1-q를 maximize
    - (1줄짜리 수식) $$ q_1^{y_1}*(1-q_1)^{1-y_1}$$을 maximize
    - (log를 취하여 표현) $$ log{q_1^{y_1}*(1-q_1)^{1-y_1}}$$
      - log를 써도 되나? 단조증가함수라 최소값, 최대값 찾는 방향이 동일하다. 써줘도 됨.
  - 총 loss
    - 첫 번째 사진과, 두 번째 사진에 대해 예측하는 것은 서로 독립사건이므로, 확률은 곱하기로 표현해야 함. 
      - **곱한 확률을 최대화해야 한다.**
        - 곱한 확률의 위험성: 0-1사이 값을 계속 곱하면 underflow
      - 로그를 취했으니 더하기로 써줄 수 있다. 
    - $$ log{(q_1^{y_1}*(1-q_1)^{1-y_1})} + log{(q_2^{y_2}*(1-q_2)^{1-y_2})} +... $$.
    - Loss의 총합, $$ \sum_n log{(q_n^{y_n}*(1-q_n)^{1-y_n})} $$ 이것을 maximize
    - -를 붙여서, $$ \sum_n -log{(q_n^{y_n}*(1-q_n)^{1-y_n})} $$ 이것을 minimize
    - 이 손실 함수를 **크로스 엔트로피 손실 함수**라고도 한다.
      - 크로스 엔트로피 손실함수를 사용하는 이유
        - 수학적 편의성: 로그가 수학적으로 다루기 편함. 특히 로그의 도함수가 분수라 편함.
        - 수치 안정성: 로그 함수는 확률 값이 0 또는 1에 가까워질 때 손실 값을 무한대로 증가시킴. 이는 모델이 잘못된 예측을 할 때 큰 패널티를 부여하게 되어 모델이 더욱 안정적으로 학습할 수 있게 도움.
        - 확률적 해석: 크로스 엔트로피 손실 함수는 두 확률 분포 간의 차이를 측정한느 방법으로 해석될 수 있음. 실제 레이블과 모델의 예측 간의 차이를 측정하는 데 이상적
        - 컨벡스성: 로지스틱 회귀의 경우, 크로스 엔트로피 손실 함수는 컨벡스 함수임. 전역 최소값을 가지며, 경사 하강법 등의 최적화 알고리즘이 전역 최소값을 효과적으로 찾을 수 있음을 의미.

- 여기서 구한 loss함수를 미분해서 learning rate곱해서 weight로부터 빼주면서 최적의 weight를 찾으면 되는 것.
  - 모든 loss함수를 구하면 gradient descent, 일부 뽑으면 minibatch.. stochastic GD, adam, etc.





## 크로스 엔트로피 손실함수

크로스 엔트로피 손실 함수(Cross-Entropy Loss Function)는 분류 문제에서 널리 사용되는 손실 함수입니다. 특히 로지스틱 회귀와 같은 이진 분류 문제나 소프트맥스 회귀와 같은 다중 클래스 분류 문제에서 주로 사용됩니다.

크로스 엔트로피는 두 확률 분포 간의 차이를 측정하는 방법으로, 머신 러닝에서는 모델의 예측 확률 분포와 실제 레이블의 확률 분포 간의 차이를 측정하는 데 사용됩니다.

1. **이진 분류에서의 크로스 엔트로피 손실 함수**:

   $$ L(y,p)=−(ylog(p)+(1−y)log(1−p))$$

   여기서 y는 실제 레이블(0 또는 1)이고, p는 모델이 예측한 확률입니다.

2. **다중 클래스 분류에서의 크로스 엔트로피 손실 함수**:

   $$ L(y,p)=−\sum_i y_i log(p_i) $$

   여기서 $$ y_i $$는 i번째 클래스의 실제 레이블(원-핫 인코딩 형식)이고, $$ p_i $$는 i번째 클래스에 대한 모델의 예측 확률입니다.

**특징 및 장점**:

- 크로스 엔트로피 손실 함수는 모델의 예측이 실제 레이블과 얼마나 잘 일치하는지를 측정합니다. 예측이 정확할수록 손실은 낮아지며, 예측이 틀릴수록 손실은 높아집니다.
- 로그 함수의 특성 때문에, 모델이 잘못된 클래스에 대해 매우 확신하는 경우 손실이 크게 증가합니다. 이는 모델이 잘못된 예측을 피하도록 도와줍니다.
- 크로스 엔트로피는 모델의 예측 확률 분포와 실제 레이블의 확률 분포 간의 차이를 직접적으로 최소화하므로, 분류 문제에서 성능이 좋습니다.
