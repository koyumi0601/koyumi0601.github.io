---
layout: single
title: "AI Deep Dive, Chapter 5. 이진 분류와 다중 분류 04. 인공신경망은 MLE 기계다"
categories: machinelearning
tags: [ML, Machine Learning, AI, AI Deep Dive]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true


---

*AI Deep Dive Note*



# Chapter 5 - 04. 인공신경망은 MLE 기계다

- loss함수를 만들 때, 모델에 적합한 likelihood 확률을 설정하고, NLL을 취하면 된다.

  - 이산확률분포일 때 
    - 이진분류 (ex. 강아지 or 고양이)의 likelihood: 베르누이 분포
    - 다중분류
  - 연속확률분포일 때 
    - (ex. 키 vs 몸무게)의 likelihood: 가우시안 분포 - 평균만 갖다쓰도록 정리하면 결국 MSE가 나온다.

  - 확률을 최대화한다로부터, loss함수의 의미를 담기 위해 -를 곱한다
  - 연산 편의를 위하여(?) log를 취한다. 단조증가라 최대최소문제에 영향 없음
    - 유도하면 MSE 나오고 하는 거보니까 단순히 수학적 계산 편의 이상의 의미인 것 같다. 뭘까

- 확률모델로부터 likelihood를 선정할 수 있고, 이를 통해 산출된 loss함수를 이용해서, gradient descent하여 weight를 업데이트 해 나가면 '학습'



## 자세한 설명

- maximum likelihood estimation

- $$ (q-1)^2 $$ vs $$ -log q $$를 최소화하는 것의 뿌리는 모두 MLE이다.
  - MSE로 나온 것도, 가우시안 분포로 가정하고 평균을 예측치로 잡으면 NLL으로 (-log를 씌워서) 유도할 수 있다.

- $$ q^y(1-q)^{1-y} $$를 다시 보자.. -> 베르누이 분포 식. 이것을 likelihood로 삼자.



## 베르누이 분포로 가정

- 베르누이 분포는 이산확률분포이다.
- 내가 입력한 사진이 강아지 혹은 고양이 뿐이라면, 베르누이 분포로 가정하는 것이 훨씬 적절하다.

- ![ch0504_1]({{site.url}}/images/$(filename)/ch0504_1.png)

![ch0504_2]({{site.url}}/images/$(filename)/ch0504_2.png)

- 이 식을 likelihood로 삼자
- $$ q^y*(1-q)^{1-y} $$.
- 여기에 NLL을 구하면 $$ -log(q^y*(1-q)^{1-y}) $$.
- n번째 사건들의 독립시행이므로 곱하면 여러 사건의 확률이다. 로그가 있으니 덧셈으로 표현이 가능하다.
-  $$ -\sum_n(log(q_n^y_n*(1-q_n)^{1-y_n})) $$.



## 가우시안 분포로 가정

- 가우시안 분포는 연속확률분포이다.
- 내가 입력한 키, 몸무게가 가우시안 분포를 따른다고 가정하면 likelihood는
- $$ \frac{1}{\sqrt{2*pi*\sigma^2}^{-\exp{\frac{(y-\hat{y})^2}{2\sigma^2}}}}$$ .
- -log 씌우고 상수 무시하고 유도하면 $$ \sum{(y_i-\hat{y_i})^2} $$ 나온다 MSE와 동일하다.
- 이걸 loss함수로 쓰면 된다.



## 사실은 둘 다 NLL(Negative Log Likelihood)

- q: 강아지일 확률을 최대화

  - 베르누이 분포
  - 최대화 식이므로, loss 함수(최소화)로 만들려면 -를 붙이고, 편의를 위해 log함수를 붙이자
  - 독립시행 확률은 확률 곱인데, sum으로 변환이 가능하다.

  





# 강의 내용 요약

1. **인공신경망의 기본 원리**:
   - 인공신경망은 최대우도추정(Maximum Likelihood Estimation, MLE)의 원리에 기반합니다.
   - 이는 딥러닝의 핵심 이론적 뿌리입니다.
2. **손실 함수의 비교**:
   - MSE (Mean Squared Error)와 -log q는 둘 다 MLE의 원리에 기반한 손실 함수입니다.
   - 이 두 손실 함수는 결국 같은 목표를 가지고 있습니다: 데이터의 likelihood를 최대화하는 것입니다.
3. **베르누이 분포와 Likelihood**:
   - 베르누이 분포의 확률 질량 함수는$$ q^y*(1−q)^{1−y} $$입니다.
   - 이 식을 likelihood로 보고, 이를 최대화하는 파라미터 w를 찾는 것이 목표입니다.
4. **학습의 목표**:
   - **Level 1**: 입력 x에 대해 출력 y와 가장 비슷한 값을 가지는 q를 찾는 것입니다. 예를 들어, 강아지 사진을 입력으로 받았을 때, q가 1에 가까워야 합니다.
   - **Level 3**: 입력 x에 대해 y의 확률을 최대화하는 q를 찾는 것입니다. 이는 y의 확률을 최대화하기 위해 가중치 w를 조절하는 것을 의미합니다.
5. **화자의 주요 메시지**:
   - 딥러닝과 인공신경망의 다양한 손실 함수나 학습 방법들은 결국 MLE의 원리에 기반하고 있습니다.
   - 이 원리를 이해하면 다양한 손실 함수나 학습 방법들을 통일된 관점에서 이해하고 해석할 수 있습니다.





# 추가 조사

- 0-9까지의 손글씨를 인식하는 문제: 다중 클래스 분류 - 다항분포 사용

- 다항분포

  - 베르누이 분포의 확장

  - ### 다항 분포의 확률 질량 함수(PMF):

    �(�1=�1,�2=�2,…,��=��)=�!�1!�2!…��!�1�1�2�2…����*P*(*X*1=*x*1,*X*2=*x*2,…,*X**k*=*x**k*)=*x*1!*x*2!…*x**k*!*n*!*p*1*x*1*p*2*x*2…*p**k**x**k*

    여기서:

    - ��*X**i*는 i번째 결과가 나타나는 횟수입니다.
    - ��*x**i*는 i번째 결과가 나타나는 횟수의 특정 값입니다.
    - ��*p**i*는 i번째 결과가 나타날 확률입니다.
    - �*n*은 전체 시도 횟수입니다.
    - �*k*는 가능한 결과의 수입니다.

    손글씨 인식 문제에서는 �=10*k*=10 (0-9까지의 숫자)이며, 각 숫자가 나타날 확률은 해당 숫자의 인식 확률입니다.

    다중 클래스 분류 문제를 해결하기 위해 신경망에서는 **소프트맥스(softmax)** 함수를 사용하여 출력을 확률로 변환합니다. 소프트맥스 함수는 각 클래스에 대한 점수(로짓)를 받아서 확률 분포로 변환합니다. 이 확률 분포는 다항 분포를 따르며, 손실 함수로는 **크로스 엔트로피 손실 함수**를 사용합니다.



## 문제 유형과 그에 적합한 모델들

1. **이진 분류(Binary Classification)**
   - **문제**: 스팸 메일 필터링, 질병 진단(환자가 특정 질병을 가지고 있는지 여부)
   - **모델**: 로지스틱 회귀(Logistic Regression), 서포트 벡터 머신(SVM), 결정 트리(Decision Tree), 랜덤 포레스트(Random Forest)
2. **다중 클래스 분류(Multi-class Classification)**
   - **문제**: 손글씨 숫자 인식, 이미지 분류
   - **모델**: 소프트맥스 회귀(Softmax Regression), 다중 레이블 결정 트리(Multi-label Decision Trees), 랜덤 포레스트, 신경망(Neural Networks)
3. **회귀(Regression)**
   - **문제**: 주택 가격 예측, 주식 가격 예측
   - **모델**: 선형 회귀(Linear Regression), 다항 회귀(Polynomial Regression), 릿지(Ridge)와 라쏘(Lasso) 회귀
4. **군집화(Clustering)**
   - **문제**: 고객 세분화, 유전자 분류
   - **모델**: K-평균(K-means), 계층적 군집화(Hierarchical Clustering), DBSCAN
5. **차원 축소(Dimensionality Reduction)**
   - **문제**: 시각화, 피처 선택
   - **모델**: 주성분 분석(PCA), t-SNE, LDA
6. **시퀀스 예측(Sequence Prediction)**
   - **문제**: 주식 가격 시계열 예측, 날씨 예측
   - **모델**: 순환 신경망(RNN), LSTM, GRU
7. **추천(Recommendation)**
   - **문제**: 영화 추천, 상품 추천
   - **모델**: 협업 필터링(Collaborative Filtering), 행렬 인수분해(Matrix Factorization), 딥러닝 기반 추천 시스템
8. **이상 탐지(Anomaly Detection)**
   - **문제**: 신용 카드 사기 탐지, 네트워크 침입 탐지
   - **모델**: One-Class SVM, Isolation Forest, 오토인코더(Autoencoders)



## 의료영상의 진단에서 사용하는 모델

1. **기본 CNN 모델**:

   - LeNet, AlexNet, VGGNet 등의 기본적인 CNN 구조가 병변 인식의 초기 모델로 사용되었습니다.
   - **의료 영상 데이터**: X-ray, MRI
   - **병변 유형**: 기본적인 종양, 결절 인식
   - **전처리**: 이미지 정규화, 리사이징
   - **데이터 증강**: 회전, 반전, 확대/축소
   - **후처리**: -

2. **ResNet (Residual Network)**:

   - 깊은 네트워크에서 발생할 수 있는 그래디언트 소실 문제를 해결하기 위해 개발된 모델로, 의료 영상 분석에도 널리 사용됩니다.
   - **의료 영상 데이터**: CT, MRI
   - **병변 유형**: 복잡한 구조의 종양, 병변 인식
   - **전처리**: 이미지 정규화
   - **데이터 증강**: 회전, 반전, 확대/축소
   - **후처리**: -

3. **U-Net**:

   - 의료 영상 분할에 특화된 모델로, 병변 영역을 정확하게 분할하는 데 사용됩니다. U-Net의 구조는 인코더와 디코더 부분으로 구성되며, skip-connection을 사용하여 특징을 보존합니다.

   - **의료 영상 데이터**: MRI, CT, 현미경 이미지, 초음파

     - U-Net은 의료 영상 분할에 널리 사용되는 모델로, 초음파 영상에서의 병변 분할에도 효과적입니다. U-Net의 구조는 이미지의 세부 정보를 잘 보존하면서 병변 영역을 정확하게 분할할 수 있게 설계되었습니다.

   - **병변 유형**: 병변 영역 분할, 조직 분할

   - **전처리**: 이미지 정규화, 리사이징

   - **데이터 증강**: 회전, 반전, 확대/축소

   - **후처리**: 분할된 영역의 후처리 (예: 모폴로지 연산)

   - 더 발전된 모델들

     - **U-Net**:
       - **개요**: U-Net은 의료 영상 분할을 위해 처음 개발된 모델입니다. 그 이름은 그 구조가 'U' 형태를 띠기 때문에 붙여졌습니다. 이 모델은 인코더와 디코더로 구성되며, 인코더는 영상의 특징을 추출하고, 디코더는 추출된 특징을 바탕으로 영상을 분할합니다.
     - **U-Net++**:
       - **개요**: U-Net++는 U-Net의 구조를 확장하여 중간 출력을 다양한 스케일에서 연결하는 방식을 도입했습니다. 이로 인해 네트워크는 다양한 스케일의 특징을 효과적으로 활용할 수 있게 되었습니다.
       - **특징**: 중간 스케일의 특징 맵들이 서로 연결되어, 네트워크가 다양한 스케일의 정보를 동시에 활용할 수 있습니다.
     - **U-Net+++**:
       - **개요**: U-Net+++는 U-Net++의 아이디어를 더욱 확장하여, 더 많은 중간 연결을 도입한 모델입니다. 이로 인해 네트워크는 더욱 다양한 스케일의 정보를 활용하면서, 복잡한 구조의 병변도 효과적으로 분할할 수 있게 되었습니다.
       - **특징**: U-Net++보다 더 많은 중간 연결을 가지며, 이로 인해 네트워크의 표현력이 향상됩니다.

     이 외에도 U-Net 기반의 다양한 변형 모델들이 연구되고 있습니다. 예를 들면, **Attention U-Net**은 Attention 메커니즘을 도입하여 중요한 영역에 집중하게 하는 방식을 사용하고, **Nested U-Net**은 중첩된 U-Net 구조를 사용하여 성능을 향상시키는 방법을 제안하였습니다.

4. **Inception (GoogLeNet)**:

   - 다양한 크기의 컨볼루션 필터를 동시에 사용하여 복잡한 패턴을 인식하는 데 효과적입니다.
   - **의료 영상 데이터**: X-ray, MRI
   - **병변 유형**: 다양한 크기와 형태의 병변 인식
   - **전처리**: 이미지 정규화
   - **데이터 증강**: 회전, 반전, 확대/축소
   - **후처리**: -

5. **Attention Mechanism**:

   - 영상 내 중요한 영역에 주목하여 병변 인식의 정확도를 향상시키는 기법입니다.
   - **의료 영상 데이터**: MRI, Ultrasound
   - **병변 유형**: 작은 병변 또는 중요한 영역의 인식
   - **전처리**: 이미지 정규화
   - **데이터 증강**: 회전, 반전
   - **후처리**: -

6. **3D CNN**:

   - 3D 영상(예: MRI, CT)에서 병변을 인식하기 위해 3차원 컨볼루션을 사용하는 모델입니다.
   - **의료 영상 데이터**: 3D MRI, 3D CT
   - **병변 유형**: 3차원 병변 인식 및 분할
   - **전처리**: 3D 이미지 정규화
   - **데이터 증강**: 3D 회전, 반전
   - **후처리**: 3D 분할된 영역의 후처리

7. **Transfer Learning**:

   - 대규모 데이터셋(예: ImageNet)에서 사전 훈련된 모델을 사용하여 의료 영상 데이터에 적용하는 기법입니다. 의료 영상 데이터는 종종 제한적이므로, 사전 훈련된 모델을 활용하여 성능을 향상시킬 수 있습니다.
   - **의료 영상 데이터**: X-ray, MRI, CT
   - **병변 유형**: 일반적인 병변 인식
   - **전처리**: 이미지 정규화, 리사이징
   - **데이터 증강**: 회전, 반전, 확대/축소
   - **후처리**: -

8. **Ensemble Methods**:

   - 여러 모델의 예측을 결합하여 병변 인식의 정확도를 향상시키는 기법입니다.
   - **의료 영상 데이터**: 모든 종류의 의료 영상
   - **병변 유형**: 다양한 병변 인식
   - **전처리**: 이미지 정규화
   - **데이터 증강**: 회전, 반전, 확대/축소
   - **후처리**: -



## 인기있는 조합

1. - **U-Net with Data Augmentation**: U-Net 모델에 다양한 데이터 증강 기법을 적용하여 모델의 일반화 성능을 향상시키는 방법.
   - **Transfer Learning with Pre-trained Models**: 이미지넷과 같은 대규모 데이터셋에서 사전 학습된 모델을 사용하여 초음파 영상에 적용하는 방법.
   - **Attention Mechanism**: 초음파 영상에서 중요한 영역에 집중하여 병변을 더 정확하게 인식하는 방법.
   - **3D U-Net**: 3D 초음파 영상에서 병변을 분할하는데 사용되는 3D 버전의 U-Net.
   - **Ensemble Methods**: 여러 모델의 예측을 결합하여 더 높은 정확도를 달성하는 방법.
   - **Generative Adversarial Networks (GANs)**: 초음파 영상 향상 및 병변 시뮬레이션에 사용되는 생성적 적대 신경망.
   - **Capsule Networks**: 병변의 공간적인 관계를 더 잘 포착하는 신경망 구조.

이러한 조합은 연구의 목적, 사용되는 데이터, 연구자의 전문성 등에 따라 다양하게 변형될 수 있습니다. 연구 논문, 학회 발표, 연구 기관의 보고서 등을 통해 최신의 연구 동향과 인기 있는 모델 조합을 확인하는 것이 좋습니다.



## 모델의 적합성 검증

1. **Train/Validation/Test Split**:
   - 데이터를 훈련, 검증, 테스트 세트로 분할합니다.
   - 모델을 훈련 세트로 학습시키고, 검증 세트로 성능을 평가합니다.
   - 여러 모델 중 가장 검증 세트에서 좋은 성능을 보이는 모델을 선택합니다.
   - 최종적으로 테스트 세트를 사용하여 선택된 모델의 성능을 평가합니다.
2. **Cross-Validation (교차 검증)**:
   - 데이터를 여러 개의 폴드(fold)로 분할합니다.
   - 각 폴드를 검증 세트로 사용하면서 나머지 폴드를 훈련 세트로 사용하여 모델을 학습 및 평가합니다.
   - 모든 폴드에 대한 평가 결과의 평균을 사용하여 모델의 성능을 평가합니다.
3. **A/B Testing**:
   - 실제 환경에서 두 개 이상의 모델을 동시에 배포하여 성능을 비교합니다.
   - 사용자나 환경에 따라 무작위로 모델 A 또는 모델 B를 사용하게 하고, 결과를 비교하여 더 나은 성능을 보이는 모델을 선택합니다.
4. **Confusion Matrix**:
   - 분류 문제에서 모델의 성능을 평가하는 데 사용됩니다.
   - 실제 레이블과 예측 레이블을 비교하여 TP, TN, FP, FN 값을 얻습니다.
   - 이를 바탕으로 정밀도, 재현율, F1 점수 등의 지표를 계산하여 모델의 성능을 평가합니다.
5. **ROC Curve and AUC**:
   - 이진 분류 문제에서 모델의 성능을 평가하는 데 사용됩니다.
   - 여러 임계값에 대한 진짜 양성 비율(TPR)과 거짓 양성 비율(FPR)을 플롯하여 ROC 곡선을 그립니다.
   - AUC (Area Under the Curve) 값은 ROC 곡선 아래의 영역을 나타내며, 모델의 성능을 평가하는 지표로 사용됩니다.
6. **Early Stopping**:
   - 모델 학습 중에 검증 세트의 성능이 개선되지 않을 때 학습을 중단하는 기법입니다.
   - 과적합을 방지하고 최적의 모델을 선택하는 데 도움을 줍니다.
